\section{Background} 

\subsection{The Transformer}

In 2017, Vaswani et al. \cite{vaswani2017attention} introduced a ground-breaking model called the "Transformer". Transformer is a model designed to solve the sequence-to-sequence mapping problem. In a sequence-to-sequence problem we have a sequence which consists of tokens\footnote{A token is a multi-dimensional numeric vector representation of an element in the sequence. Reason why sequence of tokens is used as input to a sequence-to-sequence model is because ML models can only "understand" numbers - therefore they can't work with some type of sequences (eg. human language sentences). Reason why tokens are vectors rather than simple nubers is because they are designed to represent the semantic meaning of a real-world element they represent and the semantic meaning of a real-world element could change depending on the context. Therefore different dimensions of a token account for different meaning of that element in different contexts.} coming from a finite vocabulary\footnote{The vocabulary doesn't always need to be finite, as we will see later on.} which are in a specific order, and we have an output sequence which is again a sequence of tokens in a specific order. The task of a sequence-to-sequence model is to learn the correct relationship between the input and output sequences so that when the model is presented with an unseen input sequence, it can predict what the correct output sequence is. In essence, sequence-to-sequence model's goal is to learn the "meaning" of the relationship between the input and output sequence. An example of sequence-to-sequence tasks are:
\begin{enumerate}
    \item Language translation - where an input sequence could be a sentence in our native language and the output sequence would be the correct translation of that sentence in a foreign language.
    \item Chatbots - where an input sequence could be a question and the output sequence the correct answer to that question.
    \item Time-series forecasting - where an input sequence is a certain time-series and the output sequence is the future values of that time-series. 
\end{enumerate}

Before the Transformer, there have been many ML models which attempted to solve these tasks. Sutskever et al. (2014) \cite{sutskever2014sequence} used multilayer LSTM\footnote{LSTM stands for Long Short Term Memory cell} (type of RNN\footnote{RNN stands for Recurrent Neural Network. It is a type of Artificial Neural Network architecture.} model) for this task. LSTM \cite{hochreiter1997long} and GRU\footnote{GRU stands for Gated Recurrent Unit. GRU is also a type of RNN.} \cite{cho2014learning} used to be state-of-the art sequence-to-sequence models, however they have some key issues. They require large memory\footnote{This means that it is difficult to parallelize the training process across training examples and smaller batch sizes had to be used.}, their nature is sequential\footnote{This means that it is impossible to parallelize training within the training examples.}, and they suffer from an information bottleneck due to the fixed size of the hidden state\footnote{The hidden state is an intermittent sequence within a sequence-to-sequence model. It is a product of applying the encoder on the input sequence. The output sequence is generated by applying the decoder to the hidden state. Having a hidden state of fixed size means that some information will inherently get lost when a very long input sequence is fed into the model.}. These problems were adressed by the Transformer.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{transformer.PNG}
\caption{Transformer architecture schema \cite{vaswani2017attention}}
\label{fig:transformer}
\end{figure}

Transformer consists of and Encoder and a Decoder\footnote{Similar to already mentioned RNNs.}. Figure \ref{fig:transformer} is a schematic representation of the Transformer model. Left part is the Encoder and the right part is the Decoder.

\subsubsection{Encoder}

The encoder is a stack of multiple identical layers\footnote{In the original paper there are 6 of these layers in the encoder stack}, each layer consisting of two sub-layers:

\begin{enumerate}
    \item Multi-head self-attention layer
    \item Feed-forward neural network
\end{enumerate}

Multi-head Self-attention layer consists of N so-called "attention heads"\footnote{In the Original paper, N = 8.}. An attention head is essentially a series of arithmetic operations performed on the input sequence: Let's say we have a sequence of tokens S = {s\textsubscript{1}, s\textsubscript{2}, ..., s\textsubscript{n}} the dimension of input tokens is 1 x d\textsubscript{model} \footnote{In the original paper, d(model) = 512.}. An attention head consists of weight matrices\footnote{These weight matrices are all learnable weights.} W\textsuperscript{Q}, W\textsuperscript{K} and W\textsuperscript{V} \footnote{Q, K, and V stand for Query, Key and Value.} of the dimension (d\textsubscript{model} x d\textsubscript{k}) where d\textsubscript{k}=d\textsubscript{model}/N. Multiplying these weight matrices by token s\textsubscript{i} from the input sequence creates Q\textsubscript{i}, K\textsubscript{i} and V\textsubscript{i} vectors respectively of dimension 1 x d\textsubscript{k}. For each token i, dot product of Q\textsubscript{i} and K\textsubscript{j} (for all j, 0<j<n+1) is calculated: dot\textsubscript{i, 1}, dot\textsubscript{i, 2}, ..., dot\textsubscript{i, n}. These dot-products are then scaled\footnote{Scaling factor is 1 / square root of d(k). According to the paper, this is done so that the weights are more stable during training.}, a softmax layer is applied\footnote{Applying Softmax on a series of numbers means scaling all of them so that they sum up to 1.} to all these dot products. Each dot product dot\textsubscript{i, j} then multiplies the corressponding V\textsubscript{j} to get the series of V vectors: V\textsubscript{i, 1}, V\textsubscript{i, 2}, ..., V\textsubscript{i, n}. All these V vectors are then element-wise summed up to Z\textsubscript{i} (of dimension 1 x d\textsubscript{k}) which represents the self-attention output for the token i. This is repeated for all n tokens and creates a series of vectors Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{n} (one for each token) which are then vertically stacked into matrix Z of dimension n x d\textsubscript{k}. This matrix Z\textsubscript{h} is the output of the single self-attention head h. If we vectorize this process\footnote{When we need to do the same arithmetic operations multiple times with different vectors, we can simply concatenate these vectors into matrices and do the operations just once with these matrices.}, we can express attention as \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]
 See Figure \ref{fig:scaleddotproductattention}:

\begin{figure}[h]
\centering
\includegraphics[width=6cm]{Image/scaled dot product attention.PNG}
\caption{Scaled Dot product attention \cite{vaswani2017attention}}
\label{fig:scaleddotproductattention}
\end{figure}


This process goes on in parallel in every head of the multi-head self-attention layer. Finally, the Z matrices of each self-attention head Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{N} are horizontally concatenated to produce the output of the whole multi-head self-attention layer L: Z\textsubscript{L} (of dimension n x N*d\textsubscript{k}. Multi-head self-attention layer has a weight matrix W\textsubscript{O} of dimension N*d\textsubscript{k} x d\textsubscript{model} which gets multiplied by Z\textsubscript{L} to produce the final output of the multi-head self-attention layer L.

Before the output of the multi-head self-attention layer goes into a feed-forward neural network\footnote{This network has two hidden layers with ReLU actiavation of the first layer and linear activation of the 2nd layer. In the original paper, dimensionality of hidden layers is 2048.}, a residual connection\footnote{Residual connection in this context means adding the input of the layer to the output of that layer before passing to the next layer. This is used to mitigate the vanishing gradient problem and to stabilize the training process.} \cite{he2016deep} and layer normalization\footnote{Layer normalization is a scaling technique that normalizes the the data across features within each data-sample indivisdually, as opposed to batch normalization which normalizes each feature across multiple data-samples.} \cite{ba2016layer} are applied. Residual connection and layer normalization are applied after every sub-layer of the Encoder and Decoder.  

\subsubsection{Decoder}

The Decoder has a similar structure as the Encoder with a few notable differences.

Same as the Encoder, Decoder has N layers. Each layer has three sub-layers: 

\begin{enumerate}
    \item masked self-attention layer
    \item encoder-decoder attention layer
    \item feed forward layer
\end{enumerate}
 
In principle, masked self-attention layer works similarly as the multi-head attention layer in the Encoder, however, Q, K, and V come from the already generated output\footnote{This makes the Decoder particularly suitable for autoregressive tasks as it predicts tokens sequentially based on which tokens have been predicted before, as opposed to the Encoder which would predict a sequence of tokens which it thinks would have the highest probability of being true overall without regard for the order of the sequence.}, and the attention is calculated only for the current and previously generated outputs i.e. for an already generated token i, masked self-attention head only calculates Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{i}. Otherwise, the process is the same as in the Encoder multi-head self-attention sub-layer\footnote{i.e. Encoder also stacks the outputs of each head, applies the Wo matrix, the residual connection and layer normalization.}. (See Figure \ref{fig:transformer}.)

Encoder-decoder attention layer also works similarly as the multi-head self-attention layer in the Encoder, however in this layer Q comes from the previous masked self-attention layer but K and V come from the Encoder. This allows every position in the Encoder to attend all positions in the input. (See Figure \ref{fig:transformer}.)


\subsubsection{Summary}

Even though the Attention mechanism has been around for a while \cite{bahdanau2014neural}, the self-attention model, as implemented in the Transformer, was a revolutionary step - as it solved the issue of long range dependancies\footnote{Earlier attention models had issues relating tokens which were too far apart due to attention being applied only on hidden states (hidden states are a feature of RNN models) rather than on inputs directly.}. This concept enables the model to learn the intrinsic structure of the sequence it is presented with and therefore builds a certain level of actual "understanding" of the inputs rather than simple "fitting". 

Another revolutionary aspect of the Transformer is the positional encoding. Positional encoding injects information about the absolute and relative position of each token in the sequence\footnote{this is done by sumating each input token with a positional vector of equal dimension. Value of each number in the positional vector is a sin (or could be cos) function of the position of the token it is being added to as well as of the position inside the positional vector.} thus allowing the model to learn about the importance of relative positions of tokens\footnote{This is most important on the field of NLP (Natural Language Processing) tasks.}.

Finally, and probably most importantly, the way Transformer processes inputs is inherently parallelizable - allowing significant scaling benefits to be exploited and very large models to be built. This is beneficial as larger models can learn more intricate patterns in the data and they allow for richer vector representations of the input data\footnote{Vector representations in the original Transformer had the dimension 512, whereas some modern transformer-based models support dimensions in the thousands.} \footnote{Richer representations of the input data means that the model is able to better understand more nuanced differences between some similar input tokens.}

\subsection{Transformer-based models}

Many have built on the success of the Transformer and have come up with their own models which retain many concepts from the original Transformer.

\subsubsection{NLP Transformer-based models}

Among the most famous NLP transformer-based models are the BERT (2018) \cite{devlin2018bert}, T5 (2019) \cite{roberts2019exploring} which themselves have been built upon many times (2019 alBERT \cite{lan2019albert}, 2019 roBERTa \cite{liu2019roberta}, 2019 distilBERT \cite{sanh2019distilbert}, 2020 mT5 \cite{xue2020mt5}, 2024 flan-T5 \cite{chung2024scaling}), however, OpenAI's GPT\footnote{GPT stands for Generative Pre-trained Transformer} series and LLaMA series \cite{touvron2023llama} take the crown as the most famous Transformer-based NLP models by far.

\subsubsection{CV (computer vision) Transformer-based models}

Among the most famous CV transformer-based models are 2020 ViT \cite{dosovitskiy2020image}, 2020 DETR \cite{carion2020end}, and 2021 swin-Transformer \cite{liu2021swin}.

\subsubsection{Speech and Audio processing Transformer-based models}

\subsubsection{Time-series Transformer-based models}

It was a matter of time Transformer-based architectures appeared in time-series prediction tasks. Li et al. (2019) \cite{li2019enhancing} wrote a pioneering work on transformers (LogSparse Transformer) in time-series forecasting where they adressed the issue of the quadratic space complexity\footnote{Quadratic space complexity of a model means that the ammount of working memory a model uses grows quadratically with the size of the input of the model. This is especially problematic for longer inputs for which the required memory could explode.} of the Transformer\footnote{They used heuristic approach to reduce the storage complexity to O(L*log(L)).} and proposed a more task-appropriate version of self-attention\footnote{They proposed so-called "convolutional self-attention" which brings local context awareness to Q-K mathcing}. Zhou et al. (2021) \cite{zhou2021informer} proposed a model (Informer) which also reduces computational and space complexity to O(L*log(L)) using "ProbSparse" self-attention mechanism\footnote{ProbSparse self-attention uses Query Sparsity Measurement - a metric which helps determine which Qs are more "dominant" so that the attention can be calculated for only those Qs. This is an upgrade from LogSparse Transformer, which used a heuristic to determine Which QK pairs will be calculated.} and self-attention distilling\footnote{Distilling means that the outputs of each the self-attention layers are smaller than their inputs - thus reducing the number of calculations in every subsequent layer relative to the previous one.}. Zhou et al. (2021) \cite{zhou2022fedformer} incorporate a seasonal-trend decomposition approach \cite{cleveland1990stl} \cite{wen2019robuststl}, along with Fourier analysis into the transformer-based model (FEDformer)\footnote{FED stands for Frequency Enhanced Decomposition.}. This approach saw great improvements in time-series prediction. Improvements were in terms of lower prediction errors as well as in terms of the distribution of predictions being closer to the distribution of ground-truth values according to the Kolmogorov-Smirnov distribution test\footnote{Authors of the FEDformer claim even though Informer predictions were good, they don't have the same distribution as the ground-truth time-series.} \cite{massey1951kolmogorov}.


\subsection{Time-series Foundation models (TSFM)}

Foundation Models (FM) are a class of deep models which are pre-trained on a large ammount of data - thus being able to generalize\footnote{Generalization is the ability of a model to perform well on data that it hasn't been trained on.} well as they have been taught many diverse patterns. FMs saw success on the fields of CV and NLP so it is only natural that development of FMs start developing on the time-series front. And so was the case: Since 2022, over 50 models, which can be classified as Time-series Foundation Models, have been released. If we analyze the TSFM landscape, according to the taxonomy proposed by Liang et al. (2024) \cite{liang2024foundation}, we can see that the TSFM is a very diverse class of models. TSFMs can be classified according to:

\begin{itemize}
    \item Type of time-series it is working with, which can be:
    \begin{enumerate}
        \item Standard time-series\footnote{Standard time-series is a simple sequence of any number of data-points, each associated with a time-stamp, ordered chronologically.}.
        \item Spatial time-series\footnote{Spatial time-series is a standard time-series but with a spatial dimension as well.}.
        \item Trajectory\footnote{Trajectory is a sequence of time-stamped locations which describe the movement of an object in space.}.
        \item Event sequence\footnote{Event sequence is a chronologically ordered sequence of events within a specific context.}.
    \end{enumerate}
    \item Model architecture, which could be:
    \begin{enumerate}
        \item Transformer-based.
        \item Non-Transformer-based\footnote{These models are usually MLP, RNN or CNN -bases models.}.
        \item Diffusion-based \cite{sohl2015deep} \cite{ho2020denoising}\footnote{Diffusion-based models are self-supervised models usually applied in image generation. They are trained by adding gaussian blur to the training-sample in a markov process manner and then applying (usually) CNN-based model to learn how to un-blur the same sample.}.
    \end{enumerate}
    \item The nature of the models' pre-training. which could be:
    \begin{enumerate}
        \item Pre-trained LM (Large Model)\footnote{These models use a model (usually Large Language Model) which has already been trained on some other type of sequential data, for another task and they just adopt it for time-series. Adoption strategies usually involve changing the way the inputs are tokenized in order to work better with time-series data.}.
        \item Self-supervised\footnote{Self-supervised models can be further split into Generative, Contrastive and Hybrid models. Generative models are those which have been trained to reconstruct the input data. They are particularly useful for tasks that involve generating a "missing" part of the input data (such as time-series forecasting). Contrastive models are those that have been trained to distinguish between "similar" and "dissimilar" pairs of data. As such, they are more appropriate for classification tasks (In time-series analysis, they are usually used for anomaly detection). As the name suggests, Hybrid models use mix of these two strategies.}.
        \item Fully supervised.
    \end{enumerate}
    \item Capabilities to adapt to a new time-series, which could be:
    \begin{enumerate}
        \item Fine-tuning.
        \item Zero-shot learning.
        \item Prompt engineering.
        \item Tokenization.
    \end{enumerate}
\end{itemize}

In this dissertation, I will only consider a small subset of TSFMs; ones that belong to a class of standard time-series, Transformer-based, Self-supervised (Generative) models with fine-tuning capability. However, this class of TSFMs is itself very large \cite{liang2024foundation} ( in no particular order: PatchTST \cite{nie2022time}, MOIRAI \cite{woo2024unified}, Lag-Llama \cite{rasul2023lag}, TimeSiam \cite{dong2024timesiam}, Timer \cite{liu2024timer}, TimesFM \cite{das2023decoder}, UniTS \cite{gao2024units}, TimeGPT-1 \cite{garza2023timegpt}, Chronos \cite{ansari2024chronos}, MTSMAE \cite{tang2022mtsmae}) so the efforts of this dissertation will be focused on two pioneering examples, trained on vast collection of time-series data spanning multiple domains \cite{liang2024foundation}: Lag-Llama and TimeGPT-1.



