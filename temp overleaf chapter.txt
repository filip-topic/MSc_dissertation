
\section{Introduction}

Time-series forecasting is a critical component of financial analysis, driving decision-making and resource allocation in a company. Accurate predictions of future values based on historical trends enable businesses and policymakers to mitigate risks, optimize strategies, and make informed decisions regarding asset purchases and capital allocations. In particular, time-series forecasting carries profound implications for portfolio management, hedging strategies, monetary policy formulation, and consumer credit risk assessments.

Historically, many efforts have been made to devise time-series prediction methods - many of which seeing extensive application in Finance. As the hardware technology advanced throughout the 20th and 21st century, so did the time-series models get more sophisticated. The lines are a bit blurred, but we can roughly divide the traditional time-series forecasting models into a few groups: Statistical models such as ARIMA \cite{box1970time} - which are based on some form of regression on time-series lags, Machine Learning models such as XGBoost \cite{chen2016xgboost} which are based on more complicated algorithms like gradient boosting, Artificial Neural Network (ANN) - based models and finally Deep Learning models like DeepAR \cite{salinas2020deepar}. However, the whole paradigm of Machine Learning and AI has changed in 2017 with the introduction of Transformer architecture which revolutionized the field of deep learning, initially making waves in natural language processing (NLP). Unlike recurrent architectures such as RNNs, LSTMs and GRUs, Transformers rely on self-attention mechanisms that allow them to process entire sequences simultaneously rather than sequentially. This ability to model long-range dependencies and capture intricate patterns has sparked interest in their application beyond NLP, including time-series forecasting. Time-series Foundation Models, derived from these architectures, are now emerging as powerful tools for financial forecasting, potentially outperforming traditional models in certain contexts by effectively capturing the non-linear and dynamic nature of financial markets. To better understand the motivations for focusing on TSFMs in this research, it is crucial to contextualize their development within the challenges specific to financial time-series forecasting. Traditional models, have been effective in modeling certain types of data but often struggle with the non-linearities and volatility that define financial markets. Time-series Foundation Models (TSFMs), based on Transformer architectures, have the potential to outperform these models due to their ability to capture long-term dependencies and process data in parallel. This makes them particularly well-suited to modeling the complex, dynamic patterns observed in financial data. By comparing TSFMs to traditional models, this research aims to evaluate whether TSFMs can address key limitations faced by established methods, thereby offering a more robust approach to financial forecasting.

\subsection{Research Objectives}

This dissertation aims to evaluate the performance of Time-series Foundation models in predicting financial time-series on prominent financial data such as Stock index prices, Oil prices, Exchange rates, and bank card spending volumes. The key research objectives are:

\begin{itemize}
    \item To assess the predictive accuracy of Time-series Foundation models in comparison with traditional models such as ARIMA, Prophet, and naive autoregressor.
    \item To analyze the ability of these models to capture complex patterns, seasonality, cyclicality and trend inherent in financial time-series.
    \item To provide insights into the implications of using these models in financial forecasting.
\end{itemize}

\subsection{Dissertation Organization}

This dissertation is structured into six chapters, this being the first chapter. Chapter 2 provides an in-depth exploration of the Transformer architecture, Transformer-based models, and the Transformer-led revolution of the field of AI. Chapter 3 reviews the relevant literature, with an emphasis on two prominent Time-series Foundation Models that are central to this research. These models are examined in terms of their architecture and capabilities. Chapter 4 outlines the methodology of the research. It describes the financial datasets used, the preprocessing steps, general characteristics of time-series and the experimentation process. Chapter 5 presents the results of the experiments and offers a detailed discussion on the performance of the Time-series Foundation models in comparison with traditional methods. It examines the findings in light of the research objectives and provides insights into their practical implications. Finally, Chapter 6 concludes the dissertation by summarizing the key contributions and discussing potential avenues for future work, including improvements and expansions of the research.









\section{Background} 

Appendix sections \ref{app:hist}, \ref{app:stat_models}, \ref{app:ml_models} briefly explains the history of univariate time-series forecasting, the first statistical and Machine Learning time-series prediction models.

\subsection{The Transformer}

In 2017, Vaswani et al. \cite{vaswani2017attention} introduced a ground-breaking model called the "Transformer". Transformer is a model designed to solve sequence-to-sequence mapping problems. In a sequence-to-sequence problem we have a sequence which consists of tokens (see Appendix \ref{appendix:background:token} for details about tokens) coming from a finite vocabulary\footnote{The vocabulary doesn't always need to be finite, as we will see later on.} which are in a specific order, and we have an output sequence which is again a sequence of tokens in a specific order. The task of a sequence-to-sequence model is to learn the correct relationship between the input and output sequences so that when the model is presented with an unseen input sequence, it can predict what the correct output sequence is. In essence, sequence-to-sequence model's goal is to learn the "meaning" of the relationship between the input and output sequence. An example of sequence-to-sequence tasks are:
\begin{enumerate}
    \item Language translation - where an input sequence could be a sentence in our native language and the output sequence would be the correct translation of that sentence in a foreign language.
    \item Chatbots - where an input sequence could be a question and the output sequence the correct answer to that question.
    \item Time-series forecasting - where an input sequence is a certain time-series and the output sequence is the future values of that time-series. 
\end{enumerate}

%\newpage
Before the Transformer, there have been many ML models which attempted to solve these tasks. Sutskever et al. (2014) \cite{sutskever2014sequence} used multilayer LSTM\footnote{LSTM stands for Long Short Term Memory cell} (type of RNN\footnote{RNN stands for Recurrent Neural Network. It is a type of Artificial Neural Network architecture.} model) for this task. LSTM \cite{hochreiter1997long} and GRU\footnote{GRU stands for Gated Recurrent Unit. GRU is also a type of RNN.} \cite{cho2014learning} used to be state-of-the art sequence-to-sequence models, however they have some key issues. They require large memory\footnote{This means that it is difficult to parallelize the training process across training examples and smaller batch sizes had to be used.}, their nature is sequential\footnote{This means that it is impossible to parallelize training within the training examples.}, and they suffer from an information bottleneck due to the fixed size of the hidden state (see Appendix \ref{appendix:background:hiddenstate} for hidden state explanation). These problems were adressed by the Transformer.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{transformer.PNG}
\caption{Transformer architecture schema}
\label{fig:transformer}
\end{figure}

Transformer consists of an Encoder and a Decoder. Figure \ref{fig:transformer} is a schematic representation of the Transformer model. Left part is the Encoder and the right part is the Decoder.

\subsubsection{Encoder}

The encoder is a stack of multiple identical layers\footnote{In the original paper there are 6 of these layers in the encoder stack}, each layer consisting of two sub-layers: the Multi-head self-attention layer and Feed-forward neural network. Multi-head Self-attention layer consists of N so-called "attention heads"\footnote{In the Original paper, N = 8.}. An attention head is essentially a series of arithmetic operations performed on the input sequence: Let's say we have a sequence of tokens S = {s\textsubscript{1}, s\textsubscript{2}, ..., s\textsubscript{n}} the dimension of input tokens is 1 by d\textsubscript{model} \footnote{In the original paper, d(model) = 512.}. An attention head consists of weight matrices\footnote{These weight matrices are all learnable weights.} W\textsuperscript{Q}, W\textsuperscript{K} and W\textsuperscript{V} \footnote{Q, K, and V stand for Query, Key and Value.} of the dimension (d\textsubscript{model} by d\textsubscript{k}) where d\textsubscript{k}=d\textsubscript{model}/N. Multiplying these weight matrices by token s\textsubscript{i} from the input sequence creates Q\textsubscript{i}, K\textsubscript{i} and V\textsubscript{i} vectors respectively of dimension 1 by d\textsubscript{k}. For each token i, dot product of Q\textsubscript{i} and K\textsubscript{j} (for all integer j, 0<j<n+1) is calculated: dot\textsubscript{i, 1}, dot\textsubscript{i, 2}, ..., dot\textsubscript{i, n}. These dot-products are then scaled\footnote{Scaling factor is 1 / square root of d(k). According to the paper, this is done so that the weights are more stable during training.}, and a softmax layer is applied\footnote{Applying Softmax on a series of numbers means scaling all of them so that they sum up to 1.} to all these dot products. Each dot product dot\textsubscript{i, j} then multiplies the corressponding V\textsubscript{j} to get the series of V vectors: V\textsubscript{i, 1}, V\textsubscript{i, 2}, ..., V\textsubscript{i, n}. All these V vectors are then element-wise summed up to Z\textsubscript{i} (of dimension 1 by d\textsubscript{k}) which represents the self-attention output for the token i. This is repeated for all n tokens and creates a series of vectors Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{n} (one for each token) which are then vertically stacked into matrix Z of dimension n by d\textsubscript{k}. This matrix Z is the output of that single self-attention head. If we vectorize this process\footnote{When we need to do the same arithmetic operations multiple times with different vectors, we can simply concatenate these vectors into matrices and do the operations just once with these matrices.}, we can express attention as \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\]
See Figure \ref{fig:scaleddotproductattention}:

\begin{figure}[h]
\centering
\includegraphics[width=6cm]{Image/scaled dot product attention.PNG}
\caption{Scaled Dot product attention}
\label{fig:scaleddotproductattention}
\end{figure}


This process goes on in parallel in every head of the multi-head self-attention layer. Finally, the Z matrices of each self-attention head Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{N} are horizontally concatenated to produce the output of the whole multi-head self-attention layer L: Z\textsubscript{L} (of dimension n by Nd\textsubscript{k}. Multi-head self-attention layer has a weight matrix W\textsubscript{O} of dimension Nd\textsubscript{k} by d\textsubscript{model} which gets multiplied by Z\textsubscript{L} to produce the final output of the multi-head self-attention layer L. Before the output of the multi-head self-attention layer goes into a feed-forward neural network\footnote{This network has two hidden layers with ReLU actiavation of the first layer and linear activation of the 2nd layer. In the original paper, dimensionality of hidden layers is 2048.}, a residual connection\footnote{Residual connection in this context means adding the input of the layer to the output of that layer before passing to the next layer. This is used to mitigate the vanishing gradient problem and to stabilize the training process.} \cite{he2016deep} and layer normalization\footnote{Layer normalization is a scaling technique that normalizes the the data across features within each data-sample indivisdually, as opposed to batch normalization which normalizes each feature across multiple data-samples.} \cite{ba2016layer} are applied. Residual connection and layer normalization are applied after every sub-layer of the Encoder and Decoder.  

\subsubsection{Decoder}

The Decoder has a similar structure as the Encoder with a few notable differences. Same as the Encoder, Decoder has N layers. However,  each layer has three sub-layers: Masked self-attention layer, Encoder-decoder attention layer and the Feed forward layer. In principle, masked self-attention layer works similarly as the multi-head attention layer in the Encoder, however, Q, K, and V come from the already generated output, and the attention is calculated only for the current and previously generated outputs i.e. for an already generated token i, masked self-attention head only calculates Z\textsubscript{1}, Z\textsubscript{2}, ..., Z\textsubscript{i}. Otherwise, the process is the same as in the Encoder multi-head self-attention sub-layer\footnote{i.e. Encoder also stacks the outputs of each head, applies the Wo matrix, the residual connection and layer normalization.}. This makes the Decoder particularly suitable for autoregressive tasks as it predicts tokens sequentially based on which tokens have been predicted before, as opposed to the Encoder which would predict a sequence of tokens which it thinks would have the highest probability of being true overall without regard for the order of the sequence. (See Figure \ref{fig:transformer}.)

Encoder-decoder attention layer also works similarly as the multi-head self-attention layer in the Encoder, however in this layer Q comes from the previous masked self-attention layer but K and V come from the Encoder. This allows every position in the Encoder to attend all positions in the input. (See Figure \ref{fig:transformer}.)


\subsubsection{Summary}

Even though the Attention mechanism has been around for a while \cite{bahdanau2014neural}, the self-attention model, as implemented in the Transformer, was a revolutionary step - as it solved the issue of long range dependancies\footnote{Earlier attention models had issues relating tokens which were too far apart due to attention being applied only on hidden states (hidden states are a feature of RNN models) rather than on inputs directly.}. This concept enables the model to learn the intrinsic structure of the sequence it is presented with and therefore builds a certain level of actual "understanding" of the inputs rather than simple "fitting". 

Another revolutionary aspect of the Transformer is the positional encoding. Positional encoding injects information about the absolute and relative position of each token in the sequence thus allowing the model to learn about the importance of relative positions of tokens. Positional encoding is done by summating each input token with a positional vector of equal dimension. Value of each number in the positional vector is a sin (or could be cos) function of the position of the token it is being added to as well as of the position inside the positional vector.

Finally, and probably most importantly, the way Transformer processes inputs is inherently parallelizable - allowing significant scaling benefits to be exploited and very large models to be built. This is beneficial as larger models can learn more intricate patterns in the data and they allow for richer vector representations of the input data\footnote{Vector representations in the original Transformer had the dimension 512, whereas some modern transformer-based models support dimensions in the thousands.}. Richer representations of the input data allow for better understanding of more nuanced differences between similar tokens.

\subsection{Transformer-based models}

Many have built on the success of the Transformer and have come up with their own models which retain many concepts from the original Transformer.

\subsubsection{NLP Transformer-based models}

Among the most famous NLP transformer-based models are the BERT (2018) \cite{devlin2018bert}, T5 (2019) \cite{roberts2019exploring} which themselves have been built upon many times (2019 alBERT \cite{lan2019albert}, 2019 roBERTa \cite{liu2019roberta}, 2019 distilBERT \cite{sanh2019distilbert}, 2020 mT5 \cite{xue2020mt5}, 2024 flan-T5 \cite{chung2024scaling}), however, OpenAI's GPT\footnote{GPT stands for Generative Pre-trained Transformer} series and LLaMA series \cite{touvron2023llama} take the crown as the most famous Transformer-based NLP models by far.

\subsubsection{Computer vision Transformer-based models}

Another field which saw a lot of Transformer-based work is Computer Vision (CV) where among the most famous Transformer-based models are 2020 ViT \cite{dosovitskiy2020image}, 2020 DETR \cite{carion2020end}, and 2021 swin-Transformer \cite{liu2021swin}.

\subsubsection{Time-series Transformer-based models}

It was a matter of time Transformer-based architectures appeared in time-series prediction tasks. Li et al. (2019) \cite{li2019enhancing} wrote a pioneering work on transformers (LogSparse Transformer) in time-series forecasting where they adressed the issue of the quadratic space complexity\footnote{Quadratic space complexity of a model means that the ammount of working memory a model uses grows quadratically with the size of the input of the model. This is especially problematic for longer inputs for which the required memory could explode.} of the Transformer\footnote{They used heuristic approach to reduce the storage complexity to O(L*log(L)).} and proposed a more task-appropriate version of self-attention\footnote{They proposed so-called "convolutional self-attention" which brings local context awareness to Q-K mathcing}. Zhou et al. (2021) \cite{zhou2021informer} proposed a model (Informer) which also reduces computational and space complexity to O(L*log(L)) using "ProbSparse" self-attention mechanism\footnote{ProbSparse self-attention uses Query Sparsity Measurement - a metric which helps determine which Qs are more "dominant" so that the attention can be calculated for only those Qs. This is an upgrade from LogSparse Transformer, which used a heuristic to determine Which QK pairs will be calculated.} and self-attention distilling\footnote{Distilling means that the outputs of each the self-attention layers are smaller than their inputs - thus reducing the number of calculations in every subsequent layer relative to the previous one.}. Zhou et al. (2021) \cite{zhou2022fedformer} incorporate a seasonal-trend decomposition approach \cite{cleveland1990stl} \cite{wen2019robuststl}, along with Fourier analysis into the transformer-based model (FEDformer)\footnote{FED stands for Frequency Enhanced Decomposition.}. This approach saw great improvements in time-series prediction. Improvements were in terms of lower prediction errors as well as in terms of the distribution of predictions being closer to the distribution of ground-truth values according to the Kolmogorov-Smirnov distribution test\footnote{Authors of the FEDformer claim even though Informer predictions were good, they don't have the same distribution as the ground-truth time-series.} \cite{massey1951kolmogorov}.


\subsection{Time-series Foundation models (TSFM)}

Foundation Models (FM) are a class of deep models which are pre-trained on a large ammount of data - thus being able to generalize\footnote{Generalization is the ability of a model to perform well on data that it hasn't been trained on.} well as they have been taught many diverse patterns. FMs saw success on the fields of CV and NLP so it is only natural that development of FMs start developing on the time-series front. And so was the case: Since 2022, over 50 models, which can be classified as Time-series Foundation Models, have been released. If we analyze the TSFM landscape, according to the taxonomy proposed by Liang et al. (2024) \cite{liang2024foundation}, we can see that the TSFM is a very diverse class of models. TSFMs can be classified according to:

\begin{itemize}
    \item Type of time-series it is working with, which can be:
    \begin{enumerate}
        \item \textbf{Standard time-series} - simple sequence of any number of data-points, each associated with a time-stamp, ordered chronologically.
        \item \textbf{Spatial time-series} - standard time-series but with a spatial dimension as well.
        \item \textbf{Trajectory} - a sequence of time-stamped locations which describe the movement of an object in space.
        \item \textbf{Event sequence} - a chronologically ordered sequence of events within a specific context.
    \end{enumerate}
    \item Model architecture, which could be:
    \begin{enumerate}
        \item \textbf{Transformer-based}.
        \item \textbf{Non-Transformer-based} (usually MLP, RNN or CNN -bases models).
        \item \textbf{Diffusion-based} \cite{sohl2015deep} \cite{ho2020denoising} - self-supervised models usually aused for image generation. They are trained by adding gaussian blur to the training-sample in a markov process manner and then applying (usually) CNN-based model to learn how to un-blur the same sample..
    \end{enumerate}
    \item The nature of the models' pre-training. which could be:
    \begin{enumerate}
        \item \textbf{Pre-trained LM} (Large Model). These models use an LM (usually Large Language Model) which has already been trained on some other type of sequential data, for another task and they just adopt it for time-series. Adoption strategies usually involve changing the way the inputs are tokenized in order to work better with time-series data.
        \item \textbf{Self-supervised models} - which can be further split into Generative, Contrastive and Hybrid models. Generative models are those which have been trained to reconstruct the input data. They are particularly useful for tasks that involve generating a "missing" part of the input data (such as time-series forecasting). Contrastive models are those that have been trained to distinguish between "similar" and "dissimilar" pairs of data. As such, they are more appropriate for classification tasks (In time-series analysis, they are usually used for anomaly detection). As the name suggests, Hybrid models use mix of these two strategies..
        \item \textbf{Fully supervised}.
    \end{enumerate}
    \item Capabilities to adapt to a new time-series, which could be:
    \begin{enumerate}
        \item \textbf{Fine-tuning}.
        \item \textbf{Zero-shot learning}.
        \item \textbf{Prompt engineering}.
        \item \textbf{Tokenization}.
    \end{enumerate}
\end{itemize}

In this dissertation, I will only consider a small subset of TSFMs; ones that belong to a class of standard time-series, Transformer-based, Self-supervised (Generative) models with fine-tuning capability. However, this class of TSFMs is itself very large \cite{liang2024foundation} ( in no particular order: PatchTST \cite{nie2022time}, MOIRAI \cite{woo2024unified}, Lag-Llama \cite{rasul2023lag}, TimeSiam \cite{dong2024timesiam}, Timer \cite{liu2024timer}, TimesFM \cite{das2023decoder}, UniTS \cite{gao2024units}, TimeGPT-1 \cite{garza2023timegpt}, Chronos \cite{ansari2024chronos}, MTSMAE \cite{tang2022mtsmae}) so the efforts of this dissertation will be focused on two pioneering examples, trained on vast collection of time-series data spanning multiple domains \cite{liang2024foundation}: Lag-Llama and TimeGPT-1.

The success of Transformer models in fields such as NLP and CV has sparked interest in applying these architectures to Finance, particularly for time-series forecasting. Financial time-series data presents unique challenges, such as volatility, irregular intervals, and frequent outliers, which traditional models struggle to handle. TSFMs, built on the foundations of Transformers, offer a promising alternative by leveraging self-attention mechanisms to capture both short-term fluctuations and long-term dependencies within the data. The ability of TSFMs to adapt to various time-series characteristics without requiring explicit feature engineering, makes them particularly valuable in Finance.







\section{Literature review} 


\subsection{TimeGPT-1}

TimeGPT-1 \cite{garza2023timegpt} is closed-source model - meaning that there is a lot of opacity to model's architecture, parameters and training data.

\subsubsection{Architecture}

TimeGPT-1 has encoder-decoder structure with multiple layers, each having residual connections and layer normalization. It utilizes self-attention mechanism based on the original Transformer \cite{vaswani2017attention}. 

Special capability of TimeGPT-1 is multivariate time-series forecasting - meaning that it is able to take into account "special events" and exogenous features when making predictions on a target time-series. Special events and exogenous features are time-series which are assumed to have some influence on the target time-series. Example of the former being bank holidays if the target time-series if number of flights per day and example of the latter being price of petrol if we are forecasting number of cars on the road. However, in order to do this kind of forecasting, we would have to know the actual future values of the exogenous variables which is almost impossible. TimeGPT-1 approaches this by separately forecasting the exogenous variables into the future and then basing the forecast of the target time-series on its own forecast of the exogenous features. In my opinion this is a flawed approach, inferior to simple uni-variate prediction because it brings in additional dimensions of uncertainty - whether the exogenous variables forecast are correct and whether the forecasted relationship between exogenous variables and target time-series is correct (essentially making a forecast based on a forecast). In my opinion this approach should be only used if we have high degree of certainty in forecast of future exogenous variables and the relationship between them and the target variable. 

\subsubsection{Training Data}

Authors claim TimeGPT-1 has been trained on a data-set containing over 100B data-points - the largest collection of publically availible time-series according to their knowledge. Data comes from the domains of finance, economics, demographics, healthcare, weather, IoT sensor data, energy, web traffic, sales, transport, and banking. Due to diversity of data domains, the training set contains time-series with many different characteristics: seasonalities, cycles, trends, noise, outliers. Having been trained on such a diverse dataset, TimeGPT-1 has very strong robustness and generalization capabilities.




\subsection{Lag-Llama}

\subsubsection{Tokenization}

Lag-Llama \cite{rasul2023lag} constructs "lagged features" from the past values of the time-series according to a set of frequency-dependant set of lag indices L\textsubscript{indices}={1, ..., N}. The set of lagged features of a point y\textsubscript{t} at timestamp t is then \{y\textsubscript{t-L\textsubscript{indices}[i]}, for all integers i such that 0<i<N+1\}. Lag-Llama also constructs "date-time features" F which for a point y\textsubscript{t} in time t contain information about second-of-the-minute, minute-of-the-hour, ..., month-of-the-year. Final tokenization is done by simply concatenating the date-time features and lagged features into a single vector. One of Lag-Llama's hyperparameters, "Context Length" \textit{cl} determines how many timepoints it will consider when making a prediction. When making a prediction for y\textsubscript{t+1}, Lag-Llama will use \{y\textsubscript{t-cl+1},  y\textsubscript{t-cl+2}, ..., y\textsubscript{t}\} in order to make a prediction. It is important to keep in mind that even though Lag-Llama only uses \textit{cl} previous points for making a prediction, due to how the tokenization process works - specifically the lagged features, tokens contain information from timestamps older than \textit{cl} time-points in the past.
\label{lagllama:tokenization}

\subsubsection{Architecture}

Lag-Llama is an open-source, decoder-only TSFM based on LLaMA \cite{touvron2023llama} architecture with reduced number of parameters\footnote{ Original LLaMA series comes in sizes of 6.7B, 13.0B, 32.5B and 65.2B parameters.} (2.5m) . Similarly as LLaMA, Lag-Llama utilizes concepts such as RMSnorm \cite{zhang2019root}, RoPE \cite{su2024roformer} and SwiGLU \cite{shazeer2020glu} activation function\cite{miller2024survey} (See Appendix \ref{appendix:lit_review:llama}). On top of N decoder layers, Lag-Llama has a "parametric distribution head". Parametric distribution head is a module which predicts the distribution parameters of the next prediction, given the set distribution. Authors of the paper have chosen student-t distribution for the distribution head, meaning that at inference time, the model predicts the degrees of freedom, mean and scale \cite{student1908probable} of the t-distribution from which it randomly draws n samples - idea being that this sample of n predictions gives a probabilistic insight into the model's prediction. Since the t distribution is symmetric, the mean of the distribution is equal to the mode which implies the mean is most likely to be the correct forecast - therefore it is used as the prediction. See Figure \ref{fig:lagllama}:

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{Image/lag_llama.PNG}
\caption{Lag-Llama architecture}
\label{fig:lagllama}
\end{figure}

\subsubsection{Training data}

Lag-Llama is pre-trained on diverse set of time-series domains such as energy, transportation, economics, nature, air quality and cloud operations. Only Finance-related data that was used is exchange rate data up until 2016.

\subsection{Time Series Foundation Models in Finance}

There has been work done on time-series prediction using Foundation models in Finance. However, none of it was of uni-variate nature as all used some form of exogenous data and none of it was with the use of dedicated Time Series Foundation Models. Yu et al. (2023) \cite{yu2023temporal} successfully used GPT-4 and LLaMA 13B LLMs with instruction-based fine-tuning, one-shot and few-shot inference on company profile, finance/economy news and index price data to predict NASDAQ-100 returns. However, they were predicting bins and movement direction of future values instead of actual values. Chen et al. (2023) \cite{chen2023chatgpt} used ChatGPT-informed Graph Neural Network (GNN) with prompt engineering and financial news headlines to predict direction of stock movements. Xie et al. (2023) \cite{xie2023wall} used chatGPT with chain-of-thought - enhanced zero-shot prompting with twitter data to predict stock price movement. Wimmer et al. (2023) \cite{wimmer2023leveraging} used CLIP (pre-trained CV model) \cite{radford2021learning} in combination with LSTM to predict German Stock Market movments using Open, High, Low and Close prices. To my knowledge, this is the first paper strictly examining the performance of univariate TSFMs on time-series prediction in finance.









\section{Data and Methodology} 

This Research aims to answer 3 main questions:

\begin{enumerate}
  \item How good are TSFMs, namely Lag-Llama and TimeGPT-1, on financial time-series data?
  \item In which cases do they perform better/worse?
  \item How to improve TSFMs performance?
\end{enumerate}

Methodology of this research is designed in a way to try best answer these three questions in a scientific manner.

\subsection{Time-series prediction evaluation}

Time-series predictions are evaluated using the traditional regression metrics due to the continuous nature of the target variables. All regression metrics are based on some variation of the prediction error. Prediction error is the difference between the value a model predicts and the actual (ground-truth) value. From the prediction error, following metrics are derived, and commonly used in time-series prediction tasks in Finance\cite{hu2021survey}: RMSE (Root Mean Square Error), MAE (Mean Absolute Error), MAPE (Mean Absolute Percentage Error) and R\textsuperscript{2}. An additional metric that will be used in this research is MDA (Mean Directional Accuracy) (\ref{mda}). See \cite{costantini2016forecasting} for Directional accuracy calculation. Finally, I suggest a new metric called MES (Mean Equal Sign) to be used in scenario where the time-series being predicted is financial returns. For a set of n time-series predictions \{y\textsubscript{predicted}\textsuperscript{1}, y\textsubscript{predicted}\textsuperscript{2}, ..., y\textsubscript{predicted}\textsuperscript{n}\} and ground-truth values \{y\textsubscript{actual}\textsuperscript{1}, y\textsubscript{actual}\textsuperscript{2}, ..., y\textsubscript{actual}\textsuperscript{n}\}, I define MES as mean\{sign(y\textsubscript{predicted}\textsuperscript{1} x y\textsubscript{actual}\textsuperscript{1}), sign(y\textsubscript{predicted}\textsuperscript{2} x y\textsubscript{actual}\textsuperscript{2}), ..., sign(y\textsubscript{predicted}\textsuperscript{n} x y\textsubscript{actual}\textsuperscript{n})\}. This metric is introduced because MDA alone doesn't fully capture the model's ability to predict direction of underlying asset value movement (see \ref{mes}). MDA and MES should be especially important in the field of Finance as we are not necessarily only interested in predicting the actual values of an asset, but rather we are interested in predicting the direction in which the price will go next time step.
\label{eval}

Error-based evaluation metrics are scale-dependant as the errors themselves are inherently scale-dependant. Since the experiments were ran on different types of data of different frequencies, each experiment is on a different scale, therefore, the raw evaluation metrics for different experiments cannot be used for model comparison across different time-series. It could be argued that MAPE accounts for this as it calculates absolute errors as a percentage of ground-truth actual values, however, due to different time-series having different time-series features such as noise, trend ad seasonality, some time-series are inherently more difficult to predict than the others hence the MAPE would be on a different scale on different time-series that reason. Similar case could be made for R\textsuperscript{2} as it scales the sum of square errors (SSE) by the total sum of squares (TSS), however, it also doesn't account for the fact that some time-series are inherently more predictable than the others. A perfect example of this is CHAPS data and Stock index data: most models score quite high R\textsuperscript{2} on CHAPS data (some even > 0.7), whereas pretty much all score negative R\textsuperscript{2} on Stock index data. Due to these reasons, in every experiment, models are given a rank from 1 to 7 (because there are 7 models in each experiment: 2 zero-shot TSFMs, 2 fine-tuned TSFMs and 3 benchmark models - as we will see later on) for each evaluation metric, which denotes how well each model did (relative to other models) on that specific experiment. This is a common practice in time-series results evaluation and is widely employed - most famously in the M-series competitions (prestigious time-series forecasting competition) \cite{makridakis2022m5} \cite{makridakis2000m3}. Other scale-invariant metrics such as RMSSE\footnote{Root Mean Square Scaled Error} and MASE\footnote{Mean Absolute Scaled Error} \cite{hyndman2006another} were considered, however, due to the way they are calculated, could facilitate unfair comparison if time-series are non-stationary or have heteroskedastic errors. Example of the ranking system is if a model has MSE Rank 1 on experiment E, it means that the model was the best performing model in experiment E, according to the MSE metric. Since the scale of the ranking system is constant 1-7, it can be used to compare relative model performance of the models on different time-series. However, a flaw of the ranking method is that it doesn't account for exactly how much models are better/worse than each other.



\subsection{Data}

This project uses 4 types of financial time-series (See table \ref{data:time-series}). Index price data was sourced from Yahoo Finance using yfinance Python package. Commodity and Exchange rate data was sourced using Alpha Vantage Python API. CHAPS\footnote{CHAPS is UK debit and credit card spending index.} data was sourced from UK Office for National Statistics (see Appendix \ref{data:chaps}). Depending on the frequency and the type of financial data, there is different availability of it. See table \ref{data:table} for information on which frequencies were available depending on the type of data. See table \ref{data:time-periods} for information on which time-periods were used depending on the frequency of choice. See \ref{data:exceptions} for exceptions to these tables. All in all, 56 distinct time-series were used for this research.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Stock Index} & \textbf{Commodity} & \textbf{Exchange Rate} & \textbf{CHAPS} \\ 
        \hline
        \hline
        S\&P 500 & WTI & USD/GBP & Aggregate \\ 
        \hline
        FTSE 100 &  &  & Delayable \\ 
        \hline
        DOWJ &  &  & Social \\ 
        \hline
        NASDAQ &  &  & Staple \\ 
        \hline
        &  &  & Work-related \\ 
        \hline
    \end{tabular}
    \caption{Types of data used}
    \label{data:time-series}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Frequency}} & \multicolumn{4}{c|}{\textbf{Type of Data}} \\ \cline{2-5}
        & \textbf{Stock Index} & \textbf{Commodity} & \textbf{Exchange Rate} & \textbf{CHAPS} \\ \hline
        \textbf{Daily} & YES & YES & YES & YES \\ \hline
        \textbf{Weekly} & YES & YES & YES & YES \\ \hline
        \textbf{Monthly} & YES & YES & NO & NO \\ \hline
    \end{tabular}
    \caption{Available frequencies of different types of data}
    \label{data:table}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Daily} & \textbf{Weekly} & \textbf{Monthly} \\ 
        \hline
        \hline
        2018-01-01 to 2020-01-01 & 2015-01-01 to 2020-01-01 & 1987-01-01 to 2024-01-01 \\ 
        \hline
        2020-01-01 to 2022-01-01 & 2017-01-01 to 2022-01-01 &  \\ 
        \hline
        2022-01-01 to 2024-01-01 & 2019-01-01 to 2024-01-01 &  \\ 
        \hline
    \end{tabular}
    \caption{Time periods used for different frequencies of data}
    \label{data:time-periods}
\end{table}


\subsubsection{Data Pre-processing}

In the field of Finance, there is a concept of "return". At acertain point in time T, the return R\textsubscript{T} of an asset A\textsubscript{T} is calculated as: \[R_T = (A_T-A_{T-1})/A_{T-1}\] i.e. the percentage change in the value of that asset between the points in time T-1 and T. If we think about each one of the fore-mentioned time-series as prices of an underlying asset, we can represent them as time-series of returns rather than their actual values which is a common practice in financial time-series prediction \cite{hu2021survey}. This method is not applied to CHAPS data as it wouldn't make sense given the nature of that data. Besides practical applications or working with asset returns rather than with raw values, this procedure is done to make the time-series stationary - which can be beneficial to models' performance.

\subsubsection{Time-series data characteristics}

Time-series data exhibits several key characteristics that are essential to understand for effective analysis and forecasting. \textbf{Trend} is a long-term increase or decrease in the data. It could take many shapes: none, linear, exponential, polynomial. This research uses Kendall's Tau coefficient of correlation \cite{kendall1938new}. Kendall’s Tau measures the strength of the relationship between two ordinal variables. If a time-series of length L has statistically significant Kendall Tau correlation (p-value < 0.05) with a monotonically increasing sequence \{1, 2, ..., L\}, this is a sign that there is a linear trend present in the data. If the square roots of every value in a time-series have statistically significant Kendall Tau correlation with a monotonically increasing sequence \{1, 2, ..., L\}, this is a sign that there is a quadratic trend present in the data. Same logic applies for log() values of the time-series and exponential trend.  However, since majority of our time-series contain negative values, quadratic and exponential trends weren't tested for. \textbf{Seasonality} is the presence of recurring patterns at regular intervals in the data. Chosen method for determining the presence of seasonality is by looking at partial autocorrelation (PAC) values (see \ref{pacf} for explanation) in the data and declaring a seasonal pattern present if there is a significant positive or negative partial autocorrelation present. Determining significance is this context is rather objective so I chose presence of PAC values >0.3 or <-0.3 to be significant. \textbf{Stationarity}: A time series is said to be stationary if the statistical properties such as mean and variance remain constant over time. Stationarity is determined using the Dickey-Fuller test \cite{dickey1979distribution}. Other characteristics of time-series are \textbf{Noise} and \textbf{Volatility} however, they are relatively difficult to classify and are scale dependent.
\label{ts_characteristics}

\subsubsection{Data exploration}
56 distinct time-series were used for this research. 38\% of all time-series contain linear trend, 62\% contain no trend. 46\% of time-series contain cyclical patterns and 54\% contain no discernable cyclical pattern. 88\% of time-series are stationary whereas only 12\% are non-stationary (see Figure \ref{fig:tsfeatures}). 

\begin{figure}[h]
\centering
\includegraphics[width=16cm]{Image/time-series features distribution.png}
\caption{Breakdown of all the time/series used by trend, presence of cyclical patterns and stationarity}
\label{fig:tsfeatures}
\end{figure}

Half of all time-series used are Stock indices, 27\% are CHAPS, 12\% Commodity and 11\% Exchange rate. 50\% of time-series used had daily frequency, 41\% had weekly frequency and 0.09\% were monthly See Figure \ref{fig:todfr}). 

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{Image/tod_fr distribution.png}
\caption{Breakdown of all time-series by type of data and frequency.}
\label{fig:todfr}
\end{figure}



\newpage
See Figure \ref{fig:tsfeaturesvstodfr} for breakdown of time-series features by Type of data and Frequency of data. Expected observations are that Commodity, Exchange rate and Stock index (apart from one) data are all stationary and contain no linear trend, and CHAPS data is the only type which contains some non-stationary time-series and some linear trends. This is due to the pre-processing method applied on the non-CHAPS data. 

\begin{figure}[h]
\centering
\includegraphics[width=16cm]{Image/ts_features vs tsparameters.png}
\caption{Breakdown of all time-series by time-series features, type of data and frequency.}
\label{fig:tsfeaturesvstodfr}
\end{figure}

\subsection{Experiment Design}

\subsubsection{Time-series Cross Validation (TSCV)}

In a traditional Machine Learning sense, K-fold cross-validation (CV) is a statistical technique used to evaluate the performance of a model by dividing the dataset into K equally sized subsets, or "folds." The model is trained on K-1 "train" folds and tested on the remaining "test" fold, and this process repeats K times, with each fold serving as the test set once. The results are then averaged to provide a more reliable estimate of model performance. It is important because it helps prevent overfitting by providing multiple evaluations of the model's ability to generalize to unseen data. However, in time-series prediction, where data points are temporally dependent, standard K-fold cross-validation cannot be applied directly since randomly partitioning the data breaks the time order and dependencies. To address this, K-fold cross-validation for time-series is modified by partitioning the data in a way that respects temporal structure. This allows for a valid evaluation of time-series models while still leveraging the benefits of cross-validation.


\subsubsection{Time-series Cross Validation in this research}
In this research, TSCV works the following way: say we have a time-series \textit{S} of length \textit{l}: S = \{y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{l}\}, we specified the length of the train sequence to be \textit{n}, and the length of prediction horizon to be \textit{h}. K-fold TSCV method will sample \textit{K} time-series \{F\textsubscript{1}, F\textsubscript{2}, ..., F\textsubscript{K}\} from S where each fold F\textsubscript{i} (1 <= i <= K, for all integer i) consists of the "train" and "test" part. Fold F\textsubscript{i}\textsuperscript{j} which starts from timepoint \textit{j} is defined as F\textsubscript{i}\textsuperscript{j} = \{y\textsubscript{j}, y\textsubscript{j+1}, ..., y\textsubscript{j+n-1}, y\textsubscript{j+n}, ..., y\textsubscript{j+n+h-1}\}. The model is trained on \{y\textsubscript{j}, y\textsubscript{j+1}, ..., y\textsubscript{j+n-1}\} and tested on \{y\textsubscript{j+n}, ..., y\textsubscript{j+n+h-1}\} (Size constrains being \textit{K} <= (\textit{l}-\textit{n})/\textit{h}). This research only considers next-point prediction i.e. \textit{h}=1 due to the nature of financial markets where we are primarily interested in the next value in the time-series instead of multiple future values. This research uses the version of TSCV where \textit{l} is fixed which is called "rolling window" TSCV. We employ the following fold sampling strategy: say we have two additional cross-validation parameters: \textit{f} and \textit{r}. Folds we sample from \textit{S} are split into \textit{r} groups: \{G\textsubscript{1}, G\textsubscript{2}, ..., G\textsubscript{r}\} such that all folds within a group are sequential i.e. the start-point of the following fold is the time-point right after the start-point of the preceding fold, each group containing \textit{f} folds. The first group of folds G\textsubscript{1} is always F\textsubscript{1}\textsuperscript{1}, F\textsubscript{2}\textsuperscript{2}, ..., F\textsubscript{f}\textsuperscript{f}  (i.e. first \textit{f} folds are folds with start-points 1, 2, ..., \textit{f} respectively). This sampling procedure is repeated \textit{r} times in a way that the time-distance between the first fold of G\textsubscript{k} and the first fold of G\textsubscript{k+1} is equal to the time-distance between G\textsubscript{k+1} and G\textsubscript{k+2} for all 1<=\textit{k}<=\textit{r}-2, and so that the distance between two consecutive groups is maximised subject to constraint \textit{r}\textit{f} <= (\textit{l}-\textit{n})/\textit{h} (this is essentially the same constraint as \textit{K} <= (\textit{l}-\textit{n})/\textit{h}). \textit{r} and \textit{f} parameter values used in the experiments are \textit{r}=6 (An exception to this is with the experiments ran on the monthly data where \textit{r}=8) and \textit{f}=5. In simple words: we are sampling five consecutive folds (a group), 6 times in a way that the groups are uniformly spread out over the whole time-series. See Figure \ref{fig:tscv}:

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{Image/TSCV diagram.PNG}
\caption{Schematic view of the TSCV strategy}
\label{fig:tscv}
\end{figure}


\subsubsection{Time Series Foundation Models}

Center models of this research are Lag-Llama \cite{rasul2023lag} and TimeGPT-1 \cite{garza2023timegpt}. Both models have fine-tuning capabilities with accompanying fine-tuning hyper-parameters. 

\textbf{Lag-Llama} enables the user to choose Batch size \textit{BS}, Max epochs \textit{ME}, and Learning rate \textit{LR}. Batch size refers to the number of training examples processed in one iteration before the model updates its' parameters. A smaller batch size allows for more frequent updates, while a larger batch size processes more data per update. An Epoch is a single pass through the entire fine-tuning dataset during model fine-tuning. Before a fine-tuning epoch \textit{i}: E\textsubscript{i} (1 < i <= \textit{ME}), model makes a record of all its' parameters. After E\textsubscript{i}, model's parameters have changed and  the loss\footnote{Loss in the context of fine-tuning is a measure of model’s performance on the current epoch it has trained on. Lower the loss, the better the model’s performance in that epoch.} for that epoch L\textsubscript{i} is recorded. Then after the next epoch E\textsubscript{i+1}, if L\textsubscript{i+1} < L\textsubscript{i}, then the model will keep the updated parameters from E\textsubscript{i+1}, but if  L\textsubscript{i+1} > L\textsubscript{i}, then the model will revert to the weights it had before E\textsubscript{i+1}. On E\textsubscript{1}, model parameters are updated no matter what as there is no previous loss as a reference. After E\textsubscript{1} this procedure repeats for all i <= \textit{ME}. A higher \textit{ME} means the model will have more opportunities to learn patterns from the data, but it can also risk overfitting. Learning rate is a hyperparameter that controls how much the model adjusts its weights with respect to the loss gradient; a small learning rate can lead to slow learning, while a large learning rate can cause unstable training (overshooting). These parameters together affect how efficiently and accurately a model learns from data. 
 
\textbf{TimeGPT-1} enables users only to choose number of "Fine-tune Steps" which corresponds to the number of epochs. TimeGPT uses the Adam optimizer with unknown learning hyperparameters, and batch size. Because we don't have sufficient information on TimeGPT-1 fine-tuning process and fine-tuning hyperparameters, we cannot assume the same Max epochs would be optimal for Lag-Llama and TimeGPT-1. According to its' authors \cite{garza2023timegpt}, TimeGPT-1 performance strictly improves with the number of Fine-tune Steps and reaches the plateau at around 100 Fine-tune Steps, whereas Lag-Llama reaches its' full potential with only 4 Max Epochs - as we will see later in this research. This research uses both zero-shot and fine-tuned versions of both Lag-Llama and TimeGPT-1 in order to gain insight into whether the models' performance changes with fine-tuning.


\subsubsection{Benchmark models}

Calculating the evaluation metrics for time-series prediction is not sufficient to answer whether a time-series prediction model is good or not as some time-series are inherently more and some less predictable. Therefore, the results of the TSFM evaluation need to be put into context by comparing them against widely-used benchmark models in order to make a judgment on how good they are (relative to the benchmark). Chosen benchmark models are: ARIMA (with automatic hyper-parameter selection) \cite{hyndman2008automatic}\footnote{Originally, this package is for R. I used the equivalent Python implementation: https://github.com/alkaline-ml/pmdarima.}, Naive Simple Autoregressor (NSA)\footnote{NSA prediction for the value at time-point T is the previous value at T-1.}, and Meta's Prophet model \cite{taylor2018forecasting}.


\subsubsection{Experiment}

"Running an experiment" on a certain time-series \textit{S} involves the following: We employ the TSCV technique, in a manner described before, on \textit{S}. Each fold F\textsubscript{i} (1 <= i <= \textit{rf}) represents a slice of \textit{S} with the train-set length \textit{n} and prediction horizon \textit{h}=1: F\textsubscript{i} = \{y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n}, y\textsubscript{n+1}\}. Train set T\textsubscript{i} of the Fold F\textsubscript{i} is then TR\textsubscript{i} = \{y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n}\}, and the test set is TE\textsubscript{i} = \{y\textsubscript{n+1}\}. Firstly, the whole train set TR\textsubscript{i} is used for TSFM fine-tuning. Then, only the last \textit{cl} (Context length, see Section \ref{lagllama:tokenization}) points of T\textsubscript{i}: \{y\textsubscript{n-cl+1}, y\textsubscript{n-cl+2}, ..., y\textsubscript{n}\} are used as the context window for fine-tuned TSFMs, zero-shot TSFMs and as the actual train-set for benchmark models (see Section \ref{info:disparity}) (see Figure \ref{fig:fold} for schematic view of a single TSCV fold). After all the models have made their predictions for the fold F\textsubscript{i}, their predictions are recorded along with the ground-truth actual value TE\textsubscript{i}. This is done for every fold of the TSCV process. Nuance worth mentioning is that fine-tuned Lag-Llama is getting fine-tuned only on the first fold of every group of folds (i.e. every 5 folds) whereas fine-tuned TimeGPT-1 is fine-tuned on every single fold (see Section \ref{limitation:fine-tuning}). After the models' predictions have been made and recorded for all folds, evaluation metrics for each model's predictions are calculated (see Section \ref{eval}) and also recorded. 

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{Image/fold.PNG}
\caption{Schematic view of a single fold from the TSCV technique}
\label{fig:fold}
\end{figure}


\subsubsection{Experiment configurations}

This entire research consists of running many experiments, as described above, in different experiment configurations. Experiment-configuration refers to a unique combination of experiment-parameters. Experiment-parameter is a feature of an experiment which has a certain impact on how one or more models perform. Experiment-parameters can be divided into two groups: data-parameters, and fine-tuning parameters. Data-parameters are \textit{Type of Data}, \textit{Frequency of Data} and \textit{Context Length}. Fine-tuning parameters are the following: \textit{Fine-tuning length} denotes the length of the fine-tuning set used for TSFM fine-tuning (length of the red box in Figure \ref{fig:fold}) and is applicable to both Lag-Llama and TimeGPT-1. \textit{Max Epochs}, \textit{Batch Size} and \textit{Learning Rate} are only applicable to Lag-Llama and \textit{Fine-tune Steps} is only applicable to TimeGPT-1. See Table \ref{table:experiment_parameters} for details on experiment-parameter values used. Running the experiments across many different experiment configurations allows us to analyze the results of all the experiments according to different experiment-parameters. Breaking down the results by data-parameters would answer one of the main question of this research: "In which cases do TSFMs perform better/worse?". Breaking down the results by fine-tuning parameters would answer another main question of this research: "How to improve TSFMs' performance?". Breaking down the results by two (or more) data-parameters would give more granular insight into how models perform in specific cases which is very relevant because it is unreasonable to assume model would perform the same on time-series of same frequency but different type or time-series of same type but different frequencies. In order to get full granular insight into all joint effects of experiment-parameters, experiments need to be run across every single experiment configuration. Since we have one experiment-parameter with 4 different values and seven experiment-parameters with 3 different values, this gives the total of 4x3\textsuperscript{6}=2916 different experiment-configurations\footnote{This is not even accounting for the fact that Stock index is represented by 4 different time-series and that CHAPS is represented by 5 different time-series.} to run - which is computationally unfeasible given that we run all the models, apart from TimeGPT-1, locally. 


\begin{table}[h!]
    \centering
    \resizebox{\textwidth}{!}{%
\begin{tabular}{|c||c||cccc|}
\hline
Type of   experiment-parameter & Experiment-parameters        & \multicolumn{4}{c|}{Parameter values}                                                       \\ \hline\hline
\multirow{3}{*}{Data-parameters} &
  Type of data &
  \multicolumn{1}{c|}{Stock index} &
  \multicolumn{1}{c|}{Commodity} &
  \multicolumn{1}{c|}{Exchange rate} &
  CHAPS \\ \cline{2-6} 
                               & Frequency of data & \multicolumn{1}{c|}{Daily}  & \multicolumn{1}{c|}{Weekly} & \multicolumn{1}{c|}{Monthly} &  \\ \cline{2-6} 
                               & Context length    & \multicolumn{1}{c|}{32}     & \multicolumn{1}{c|}{64}     & \multicolumn{1}{c|}{128}     &  \\ \hline\hline
\multirow{5}{*}{Fine-tuning hyper-parameters} &
  Fine-tuning length &
  \multicolumn{1}{c|}{200} &
  \multicolumn{1}{c|}{128} &
  \multicolumn{1}{c|}{64} &
   \\ \cline{2-6} 
                               & Batch size        & \multicolumn{1}{c|}{5}      & \multicolumn{1}{c|}{10}     & \multicolumn{1}{c|}{20}      &  \\ \cline{2-6} 
                               & Max epochs        & \multicolumn{1}{c|}{4}      & \multicolumn{1}{c|}{8}      & \multicolumn{1}{c|}{16}      &  \\ \cline{2-6} 
                               & Learning rate     & \multicolumn{1}{c|}{0.0005} & \multicolumn{1}{c|}{0.005}  & \multicolumn{1}{c|}{0.00005} &  \\ \cline{2-6} 
                               & Fine-tune steps   & \multicolumn{1}{c|}{100}    & \multicolumn{1}{c|}{}       & \multicolumn{1}{c|}{}        &  \\ \hline
\end{tabular}%
}
    \caption{Different experiment parameters and their values}
    \label{table:experiment_parameters}
\end{table}

\subsubsection{Data-parameter experimentation phase}

Since we are computationally restricted, we divide the whole experimentation process into phases, the first phase being the Data-parameter experimentation phase. In this phase, we employ a heuristic approach to experimentation where we keep all the fine-tuning hyper-parameters (last 5 rows of Table \ref{table:experiment_parameters}) fixed (respective fine-tuning hyper-parameter values being the ones in the first column of the last four rows of Table \ref{table:experiment_parameters}) and run experiments with different combinations of data-parameters. See Figure \ref{fig:exploration}:
\label{data experimentation phase}

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{exploration phase.PNG}
\caption{Schematic view of the Data-parameter experimentation phase}
\label{fig:exploration}
\end{figure}

\subsubsection{Aggregation phase}

After having recorded and aggregated the results (model evaluations) and experiment-configurations in the Data-parameter experimentation phase, we group the aggregated results by any single, or combination of data-parameters and take the mean of the evaluation metric ranks (according to Section \ref{eval}) in order to understand how models perform relative to each other, with different data-parameters. This phase constitutes the main body of the whole research.
\label{data aggregation phase}

One drawback of this whole research is that it was conducted on a relatively small collection of time-series of few different types (see limitations in Sections \ref{limitation:data_availability} and \ref{limitations:computation}) hence with the analysis described so far, it is difficult to extrapolate any conclusions for types of data or frequencies which we haven't used in this experiment. A potential solution to this problem is to look at underlying characteristics of the time-series that were used in this experiment (as described in Section \ref{ts_characteristics}) because these characteristics are the factors which fundamentally influence the performance of time-series prediction models. Time-series used in this research were classified according to presence of a trend, stationarity and presence of cyclical pattern. Breaking down the results of all experiments by these features can provide an insight into which underlying characteristics of time-series cause the models to perform well or poor. By incorporating this type of analysis into this research, a future user of TSFMs can simply perform tests to determine the mentioned characteristis of a time-series they want to predict (whether or not that time-series was used in this research or not), and this by consulting this research, they will know whether they should use TSFMs or not.

\subsubsection{Fine-tuning parameter experimentation phase}
 
Last phase in the in the heuristic experimentation approach is to pick (fix) a single data-configuration and run different experiment configurations with fixed data-configuration and variable model-configuration. The strategy for choosing the data-configuration for each TSFM's fine-tuning hyper-parameter optimization is by taking the data-configuration on which the TSFM performed really well on, but with some room for improvement in order to be able to tell whether the TSFM can improve with different fine-tuninghyperparameters. After we have chosen the  data-configuration for Lag-Llama, we keep it fixed and run experiments across different Model-parameter configurations. Conceptually, this process is identical to the process in the Data-parameter experimentation phase, but this time the data-parameters are fixed, and we vary the fine-tuning hyper-parameters in each experiment. Important to note is that in this phase we only experiment with Lag-Llama fine-tuning parameters (see Section \ref{limitation:API})

\subsubsection{Model-parameter aggregation phase}
Similarly as in the Aggregation phase, we can group the aggregated results of the Fine-tuning parameter experimentation phase by fine-tuning parameters and take the mean of the evaluation metric ranks in order to understand Lag-Llama performance changes with different fine-tuning parameters on the data-configuration that we chose. Doing this type of analysis enables us to answer the question "How to improve TSFMs' performance?" i.e. gives insight into which fine-tuning parameters it works best.








\section{Results and Discussion} 

\subsection{Results}

As outlined in Section \ref{eval}, results of the Data-parameter experimentation phase (Section \ref{data experimentation phase}) are evaluated using the metric ranking system and aggregated as outlined in Section \ref{data aggregation phase}. These results can be found in this section in Figures \ref{table:results_exploration_overall} to \ref{table:results_cyclicality}. Results are broken down by different Data-parameters and time-series characteristics. For each combination of Data-parameters / time-series characteristics, the best model's performance has been highlighted in purple according to each individual evaluation metric average rank. In some results tables, highlight is missing from the MES column as few different models have the same performance in that group and highlighting it would bring confusion. 

\textbf{Data-parameter exploration phase results} sub-section shows the main body of this project's research. However it is important to stress that the results from Model-parameter experimentation phase are not included in this section. The reason for that is because the specific time-series chosen for Model-parameter experimentation phase is specifically chosen because of Lag-Llama's great performance on that specific time-series. Hence, adding the results from this set of experiments to experiments from Data-parameter experimentation phase would bring bias to the whole corpus of results. Results from Model-parameter experimentation phase are separated from the results of Data-parameter experimentation phase and are adressed separately in Section \ref{results model parameter experimentation}. 

\subsubsection{Data-parameter experimentation phase results}

Overall, across all experiments in this phase (see Table \ref{table:results_exploration_overall}), ARIMA model performs the best according to all metrics apart from MDA - according to which, fine-tuned Lag-Llama is the best. This pattern repeats if we break down the results by the Context length (see Table \ref{table:results_clts}). 

Across all \textbf{Context lengths} (128, 32 and 64), ARIMA performs best according to all metrics apart from MDA, where fine-tuned Lag-Llama performs best when Context length is 128 (mean rank 2.79) or 32 (mean rank 2.95) and zero-shot Lag-Llama is the best model according to MDA with Context length of 64 (mean rank 2.84). 

Breaking down the results by \textbf{Frequency} of data (see Table \ref{table:results_freq}), ARIMA performs best on daily data according to all metrics (mean rank between 1.11 and 2.44) apart from MDA, but fine-tuned Lag-Llama comes on top with monthly (mean rank between 1.33 and 2.0) and weekly (mean rank between 2.94 and 3.01) data - leading in R\textsuperscript{2}, MSE, MAE, RMSE in both of those scenarios with zero-shot Lag-Llama being the best according to MDA (mean rank 2.52 on weekly and 2.2 on monthly data) and ARIMA being the best in MES. 

Breaking down the results by \textbf{Type of data} (see Table \ref{table:results_type}), Fine-tuned Lag-Llama tends to be the best model on Commodity, Exchange rate and Stock index types of data according to R\textsuperscript{2}, MSE, MAE, RMSE (mean ranks between 1.67 and 2.78). However ARIMA comes equal or better on Stock index in terms of MAE, MAPE and MES (mean ranks between 1.15 and 2.49), and on Exchange rate in terms of MAPE (2.11) and MES (1.33). 

Breaking down the results of \textbf{daily} frequency by \textbf{Type of data} (see Table \ref{table:results_daily_type}), simple naive autoregressor tends to be the best model on CHAPS (mean ranks between 1.43 and 1.90), fine-tuned Lag-Llama tends to be the best on Commodity (mean ranks between 2.11 and 2.89) and ARIMA tends to be best on Stock index (mean ranks between 1.17 and 2.14). On exchange rates, it isn't clear whether ARIMA or Prophet are the best. 

Breaking down the results of \textbf{weekly} frequency by \textbf{Type of data} (see Table \ref{table:results_weekly_type}), fine-tuned Lag-Llama tends to dominate other models on commodity (mean ranks between 1.0 and 2.0), Exchange rate (mean ranks between 1.22 and 1.67) and Stock index (mean ranks between 1.83 and 1.94), with ARIMA consistently proving itself the best on MES. On CHAPS data, it is unclear whether fine-tuned TimeGPT-1 or the autoregressor is the best as bost score best in different metrics.

Breaking down the results of \textbf{monthly} frequency by \textbf{Type of data} (see Table \ref{table:results_monthly_type}), it is difficult to say which is the best model on commodity type of data as few different models perform best according to different metrics. However, fine-tuned Lag-Llama, significantly outperforms other models on Stock index type of data (mean ranks between 1.17 and 2.17), with zero-shot Lag-Llama being tied first on MDA and ARIMA beating Lag-Llama only on MES.

Breaking down the results by the \textbf{stationarity} of the time-series (see Table \ref{table:results_stationarity}), simple naive autoregressor performs best on non-stationary data (mean ranks between 1.76 and 2.95) and ARIMA performs best on stationary data (mean ranks between 1.27 and 2.65). Fine-tuned Lag-Llama's is the best model by MDA (2.78) on stationary data. An interesting observation is that Lag-Llama's relative performance is significantly better on stationary data, while TimeGPT-1 relative performance is significantly better on non-stationary data. 

Breaking down the results by \textbf{presence of a linear trend} (see Table \ref{table:results_trend}), autoregressor again dominates the time-series on data with linear trend (mean ranks between 1.87 and 2.71). However, fine-tuned Lag-Llama is the best model on time-series with no trend present (mean ranks between 2.28 and 2.35) - exception being ARIMA which beats fine-tuned Lag-Llama in MAPE (2.25) and MES (1.32).

Breaking down the results by \textbf{presence of a cyclical pattern} (see Table \ref{table:results_cyclicality}), fine-tuned Lag-Llama is the best model when there isn't a cyclical pattern present (mean ranks between 2.26 and 2.42) (with ARIMA beating in only in MAPE (1.97) and MES (1.36)). On time-series with a cyclical pattern, ARIMA is the best model (mean ranks between 1.09 and 2.95) apart from fine-tuned TimeGPT-1 which beats it in MDA (3.26).


\newpage
\input{result_tables}

\clearpage

\subsubsection{Model-parameter experimentation phase results}
Time-series chosen for Lag-Llama fine-tuning hyper-parameter optimization was Commodity of weekly frequency, period of 01-01-2019 untill 01-01-2024, and Context length of 32. Since there are 4 Lag-Llama fine-tuning parameters we are experimenting with (see Table \ref{table:experiment_parameters}), each having 3 possible values,  3\textsuperscript{4} = 81 experiments were conducted.

Since working on a single time-series we can look at the actual performance metrics of fine-tuned Lag-Llama instead of the ranks. Breaking down the fine-tuning experimentation results by Batch size (see Table \ref{results:batch_size}), we can tell that Lag-Llama benefits from smaller batch size - arguably\footnote{Higher batch sizes only outperformed the low batch size in terms of MDA and MES.} performing best with Batch size of 5 (15.47\% better than other Batch sizes in terms of RMSE), which isn't surprising as lower batch sizes mean more frequent model-weight updates hence more "granular" learning. Breaking down the performance by length of the fine-tune set (see Table \ref{results:ftl}), we can tell that Lag-Llama benefits from higher fine-tune lengths as its' performance was arguably best with the highest fine-tune length: 200 (In terms of RMSE, 16.60\% better than Fine-tune length of 128 and 94.41\% bettern than Fine-tune length of 64). Table \ref{results:me} suggests that Lag-Llama performs better with lower Max epochs as it performed overall better with 4 Max epochs than with 8 or 16 (in terms of RMSE, 15.47\% better than both other cases). Finally, regarding the Leaning rate (see Table \ref{results:lr}), we cannot tell certainly which learning rate works best for Lag-Llama as it seems to have performed better with the lower (0.00005) and higher one (0.005) than with the default Learning rate of 0.0005.

\label{results model parameter experimentation}



\newpage
\input{ft_result_tables}
\clearpage



\subsection{Limitations}

\subsubsection{Information disparity between fine-tuned TSFMs and benchmark models + zero-shot TSFMs}

Theoretically, fine-tuned TSFMs are systematically advantaged over benchmark models and zero-shot TSFMs as they have access to more information as the set that they are fine-tuned on is larger than the size of the context window which zero-shot TSFMs and benchmark models use (they are using the entire train set TR\textsubscript{i} of the fold F\textsubscript{i} for fine-tuning, whereas the benchmark models and zero-shot only have access to the last \textit{cl} data-points from TR\textsubscript{i}). However, this is unavoidable as the technical implementation of fine-tuning in TSFMs is such that the length of the fine-tuning set has to be larger than its' context length - so there is bound to be information disparity between zero-shot and fine-tuned TSFMs. If we simply let benchmark models train on the whole TR\textsubscript{i}, then zero-shot TSFMs would be systematically disadvantaged in comparison to benchmark models as they would have access to less information than them. The final option is to let zero-shot TSFMs have the context length of same length as the length of the fine-tuning set, however, then the comparison between zero-shot and fine-tuned TSFMs wouldn't be appropriate as fine-tuned TSFMs would have shorter context lengths and the amount of direct information going into zero-shot and fine-tuned TSFMs would be different.
\label{info:disparity}

\subsubsection{Statistical significance}
Unfortunately, due to the nature of this experiment, it is difficult to quantify the statistical significance of the results within time-series. The way that statistical significance of time-series prediction evaluation metrics is calculated is by looking at their performance across different time-series and then calculating the mean and variance of the performance metrics and then calculating confidence intervals. In this case, this doesn't make much sense because, as we saw, time-series used in this research are inherently very diverse in terms of their characteristics, and there simply isn't enough time-series (see Section \ref{limitations:computation}) with similar characteristics so that hypothesis testing could be done. Instead, reaching statistical significance has been attempted through the use of the TSCV (see Figure \ref{fig:tscv}) where the 6 groups of 5 individual predictions are spread uniformly across each time-serie in order to cover the largest period possible so that the likelihood of getting an accidental localized result is minimized.

\subsubsection{Computational resources limitations}

The main limitation of this research is the lacking computational resources. The code which runs the experiments takes a relatively long time to execute (around 10-45 minutes for each experiment depending on Max epochs and Batch size) because Lag-Llama is run locally. This limitation led to multiple "shortcuts" being taken in this research process - such as: 

\begin{itemize}
    \item Limiting the number of time-series used in this research.
    \item Limiting the parameter-space across which was experimented.
    \item Using the heuristic approach with fixed model-parameters and varying data-parameters, and vice-versa in the Model-parameter experimentation phase.
    \item Not fine-tuning Lag-Llama before every prediction but rather every five predictions. (see Section \ref{limitation:fine-tuning})
    \item Not conducting multiple repeats of the same experiment in order to gain statistical confidence in results.
    \item Doing "only" 30 prediction per each time-series.
\end{itemize}
\label{limitations:computation}


\subsubsection{TimeGPT-1 API calls}

Second limitation of this research is the TimeGPT-1 API call limit. With paid subscription, it is limited to 10,000 calls per month - which was raised to 30,000 per my request. However, 30,000 was not enough to fully conduct the research. Each TimeGPT-1 prediction requires 2 API calls - which means that per experiment we are using 4 API calls (2 for zero-shot TimeGPT-1 and 2 for fine-tuned version). 4 calls per prediction times 30 predictions equals 120 API calls. This means that with the 30,000 limit, I was able to make 250 experiments. After the code testing and a few failed attempts, there was barely enough API calls to finish the research up to this level. due to this, unfortunately few avenues of research had to be left out such as timeGPT-1 fine-tuning parameter optimization and timeGPT-1 exogenous variables research.
\label{limitation:API}

\subsubsection{TimeGPT-1 pre-training data}

Third limitation is that we don't have public information on timeGPT-1 pre-training data and therefore we cannot be sure that timeGPT-1 wasn't already pre-trained on the data we used to evaluate its' performance (data leakage). I sent a query to timeGPT-1 team (Nixtla labs) but they refused to disclose the information. However, judging by timeGPT-1 overall performance, it doesn't seam it has already been pre-trained on the data which was used. One solution to this would've been running the experiments only on most recent data, however, this was subject to the API limitation and data availability limitation.

\subsubsection{Data availability}

Fourth limitation of this research is data availability. AlphaVantage API doesn't provide monthly Exchange rate data before 2015 which isn't enough to conduct experiments on. CHAPS monthly data isn't available before 2020 which also wasn't enough to conduct proper experiment. In addition Alphavantage API doesn't provide commodity price information for commodities of higher than monthly frequency unless for WTI and BRENT.

\label{limitation:data_availability}

\subsubsection{Fine-tuning disparity between Lag-Llama and TimeGPT-1}

Theoretically, fine-tuned versions of TSFMs should be fine-tuned on every single fold as there being distance between the fine-tuning data and the timepoint of prediction increases the likelihood that the model is learning outdated patterns. However, as Lag-Llama is run locally, fine-tuning process is taking a very long time and it was unfeasible to fine-tune it before every single prediction. So a compromise had to be made and fine-tune it only every 5 predictions.
\label{limitation:fine-tuning}


\subsection{Discussion}

\subsubsection{Overall results discussion}

\textbf{Fine-tuned Lag-Llama} performs best on data which are of Commodity, Exchange rate and Stock index type (see Table \ref{table:results_type}). This can be explained through the features of these types of data and frequencies. Looking at Figure \ref{fig:tsfeaturesvstodfr}, we can see those types of data are the ones that mostly do not contain cyclical patterns - Commodity containing the least cyclical patterns of them all and also being the type of data on which fine-tuned Lag-Llama performs best on in terms of the rank difference between it and the next best model\footnote{Difference (RMSE) between fine-tuned Lag-Llama and next best model was 0.81 on commodity, 0.05 on exchange rate and 0.05  on stock index.}. We are seeing a similar kind of pattern with results breakdown by Frequency. Lag-Llama is the best model on data of monthly and weekly frequencies (see Table \ref{table:results_freq}) - which if we look at Figure \ref{fig:tsfeaturesvstodfr} we can see are the frequencies with least cyclical patterns. Similarly as before - the frequency which has less cyclical patterns: monthly is the one on which fine-tuned Lag-Llama performs comparatively better on than of weekly frequency\footnote{Difference (RMSE) between fine-tuned Lag-Llama and next best model was 0.87 on monthly data but only 0.15 on weekly data.}. These facts point to a conclusion that fine-tuned Lag-Llama performs really well on data with no cyclical pattern and quite poorly on data with a cyclical pattern. This is further evidenced in Table \ref{table:results_cyclicality} where we see fine-tuned Lag-Llama is arguably the best model on non-cyclical time series and one of the worst on cyclical time series. If we look at trend, from Type of data - point of view, evidence is very strong that presence of linear trend negatively affects the performance of Fine-tuned Lag-Llama as Commodity, Exchange rate and Stock index almost all contain no trend and fine-tuned Lag-Llama performs excellent. From Frequency perspective, we can conclude the same as monthly frequency contains no trend and fine-tuned Lag-Llama performs by far the best on that data. Where daily frequency in almost 40\% cases contains linear trend and it also seems to be the frequency on which fine-tuned Lag-Llama performs worst. Same case can be said for stationarity of time-series.

\textbf{TimeGPT-1} models' performance is a little bit disappointing. Not in many cases they outperform other models and in the cases when they do, it is hard to find pattern which could explain why they do - leading to believe that their good performance on those specific occasions could be accidental. Only time TimeGPT model appears the best model (fine-tuned version) in more than one metric is in the weekly CHAPS data - scoring 1.93 in R\textsuperscript{2}, MSE and RMSE, whereas the 2nd best model came close behind and scored 2.53 in those respective metrics. An interesting observation is that when breaking down the results by time-series characteristics (Tables \ref{table:results_stationarity} to \ref{table:results_cyclicality}), effect of time-series characteristics on fine-tuned TimeGPT-1 performance is diametrically opposite than on fine-tuned Lag-Llama. Whereas fine-tuned Lag-Llama works relatively better with stationary data, fine-tuned TimeGPT-1 works relatively better with non-stationary data. Same goes for cyclicality and trend.

\subsubsection{General Lag-Llama great MDA performance}
Another interesting observation is that both fine-tuned and zero-shot versions of Lag-Llama perform quite well in terms MDA across the board  with fine-tuned Lag-Llama being the best overall (see Table \ref{table:results_exploration_overall}), and either one of them being the best in all cases of Context length (Table \ref{table:results_clts}), with monthly and weekly frequencies (Table \ref{table:results_freq}), Commodity, Exchange rate and Stock index data (Table \ref{table:results_type}). From the previous paragraph, we know that fine-tuned Lag-Llama performs best with stationary data with no trend and no cyclical patterns. If we look at MDA metric only, we can see that in all those cases (Tables \ref{table:results_stationarity}, \ref{table:results_trend}, \ref{table:results_cyclicality}) fine-tuned Lag-Llama is the best model by each of those time-series characteristics, and zero-shot Lag-Llama is always the 2nd best model. This is unlikely to be a coincidence and likely points to the fact that general Lag-Llama model has superior capability to predict to direction of the movement of a time-series given stationarity, no trend and no cyclical pattern.


\subsubsection{Inter-model fine-tuning performance discussion}
Across all results, Lag-Llama seems to universally benefit from fine-tuning, only exception to this being daily and weekly CHAPS and daily Exchange rate data where zero-shot and fine-tuned versions performs actually better (Tables \ref{table:results_daily_type}, \ref{table:results_weekly_type}). Looking at time-series characteristics, zero-shot Lag-Llama performs better than fine-tuned version on non-stationary data (Table \ref{table:results_stationarity}), on data with linear trend (Table \ref{table:results_trend}), and almost equally on time-series with a cyclical pattern (Table \ref{table:results_cyclicality}). This is a very interesting because one would expect that generally a model's performance would generally improve with fine-tuning, however, according to these results, on time-series with these characteristics; Lag-Llama's performance actually worsens with fine-tuning. This phenomenon brings sheds some light on Lag-Llama's fine-tuning capabilities and shows which kind of characteristics it is able to learn (stationarity, no trend, no cyclicality) and which it isn't (non-stationarity, linear trend and cyclicality). Overall, TimeGPT seems to benefit from fine-tuning only slightly. Only cases where TimeGPT-1 benefits significantly from fine-tuning is on weekly Exchange rate data, and all monthly data (Tables \ref{table:results_weekly_type}, \ref{table:results_monthly_type}). Zero-shot TimeGPT-1 even beat the fine-tuned version on daily CHAPS, daily and weekly Commodity data and weekly Stock index data (Tables \ref{table:results_daily_type}, \ref{table:results_weekly_type}). A very intriguing observation is that on time-series characteristics where fine-tuned TimeGPT performs well compared to other models (non-stationary and linear trend), the zero-shot TimeGPT-1 actually outperforms fine-tuned version (Tables \ref{table:results_stationarity}, \ref{table:results_trend}) and comes very close on cyclical data (Table \ref{table:results_cyclicality}). This is complete opposite fine-tuning behaviour than Lag-Llama's whos' fine-tuned version was dominating the zero-shot version on characteristics it was good at (stationary, no trend, no seasonality) compared to characteristics it was bad at.

\subsubsection{Lag-Llama fine-tuning results discussion}

Fine-tuning - wise, Lag-Llama seems to perform best with higher fine-tune lengths, and lower Batch sizes and Max epochs. However there are a few caveats. Fine-tune length of 128 produced a better R\textsuperscript{2} (73.91\% better) than the fine-tune length of 200 meaning that increasing the fine-tune length ad infinitum might not result in strictly better performance. This is likely because the nature of financial time-series data. Patterns don't stay constant over time and having longer fine-tuning lenghts means the model is learning older and less relevant patterns which might have explained why fine-tune length of 128 was at least in one metric better than fine-tune length of 200. This extends to the fine-tune length of 64 which had significantly higher MES than both 128 and 200 fine-tune lengths. Max epochs - wise, in theory higher Max epochs shouldn't lead to detriment in performance as the model discards the new weights in every epoch if that epoch's loss was higher than pervious epoch's. Even though it is not certain, having higher Max epochs could give the model more chances to overfit the fine-tuning data - hence make worse predictions. 

\subsubsection{Deeper investigation of Lag-Llama fine-tuning results}

Last interesting observation of the fine-tuning results is that There are a lot of results that look very similar. Looking at the Tables \ref{results:batch_size} and \ref{results:me}, they look nearly identical with some minor differences in the MDA column. This could be an indication that there were mistakes in results recording process, however, after detailed investigation, I concluded that there was no mistake, rather an interesting phenomena was occuring. In majority of the experiments in this set, Lag-Llama was making predictions which were in the order of magnitude x10\textsuperscript{-14} (near zero) (see Figure \ref{fig:lag_llama_predictions}). In every single experiment, it did make different predictions because it had a different set of fine-tuning parameters, however, because the scale of the predictions was so small, the differences in the predictions were often only in the 14th, 15th, 16th decimal place - meaning that they would have the same evaluation metrics if we're considering only 4 decimal places (see Tables \ref{results:batch_size} to \ref{results:lr}). Lekely reason for that is that weekly WTI returns time-serie looks inherently predictable, therefore, model tries to avoid maing large mistakes therefore makes safe predictions. Furthermode at around 1/3 mark of the time-series, there was a period of very large outliers. Where WTI returns sharply fell and then rose to over 350\% (see Figure \ref{fig:wti_returns}). Having used this volatile time-period in fine-tuning im nost cases (depending on fine-tune-length and which fold of the TSCV), it is likely that the model-weights got stuck in a local minimum - hence similar predictions - hence near identical evaluation metrics in many cases. However, even though the the model's predictions have degraded to predicting near-0 all the time, we can still see that in every single scenario, the model has >50\% MDA, meaning that it has more than 50\% chance of predicting the correct direction of the time-series movement in every configuration of fine-tuning parameters.

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{Image/lag_llama predictions.png}
\caption{Predictions of Lag-Llama with Batch size 5, fine-tune length 200, Max epochs 8 and Learning rate 0.0005 on weekly WTI returns}
\label{fig:lag_llama_predictions}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=12cm]{Image/wti_weekly_returns.png}
\caption{WTI weekly returns between 2019-01-01 and 2024-01-01}
\label{fig:wti_returns}
\end{figure}












 






